'''path_segmentation'''

#%%

# pylint: disable=E0401
# pylint: disable=C0103

# Import. Here, we import tensorflow, which gives us access to the library.
import os
import tensorflow as tf

# Here, we define helper functions for writing data to an example to a
# TFRecord file.

def float_feature(value):
    '''Create float_list-based feature'''
    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))


def int_feature(value):
    '''Create int64_list-based feature'''
    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))


def bytes_feature(value):
    '''Create bytes_list-based feature'''
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

# Here, we define a function that makes an example, which is written to a
# TFRecord file.
def make_example(feature, label):
    '''Make example from feature and label'''

    # Here, the feature and label are decoded PNG images.

    feature_raw = feature.tostring()
    label_raw = label.tostring()

    example = tf.train.Example(features=tf.train.Features(feature={
        'feature_height': int_feature(feature.shape[0]),
        'feature_width': int_feature(feature.shape[1]),
        'feature_depth': int_feature(feature.shape[2]),
        'label_height': int_feature(label.shape[0]),
        'label_width': int_feature(label.shape[1]),
        'label_depth': int_feature(label.shape[2]),
        'feature_raw': bytes_feature(feature_raw),
        'label_raw': bytes_feature(label_raw),
    }))

    return example

# Here, we define a function that reads a TFRecord file; parsing a single
# example.
def read_record(filename_queue):
    '''Read record'''

    # Here, the record contains examples derived from PNG images.

    reader = tf.TFRecordReader()
    _, serialised_example = reader.read(filename_queue)

    example = tf.parse_single_example(
        serialised_example,
        features={
            'feature_height': tf.FixedLenFeature([], tf.int64),
            'feature_width': tf.FixedLenFeature([], tf.int64),
            'feature_depth': tf.FixedLenFeature([], tf.int64),
            'label_height': tf.FixedLenFeature([], tf.int64),
            'label_width': tf.FixedLenFeature([], tf.int64),
            'label_depth': tf.FixedLenFeature([], tf.int64),
            'feature_raw': tf.FixedLenFeature([], tf.string),
            'label_raw': tf.FixedLenFeature([], tf.string),
        }
    )

    return example

# Here, we extract data from a single example.
def extract_example_data(example):
    '''Extract example's data'''

    feature_height = tf.cast(example['feature_height'], tf.int32)
    feature_width = tf.cast(example['feature_width'], tf.int32)
    feature_depth = tf.cast(example['feature_depth'], tf.int32)
    label_height = tf.cast(example['label_height'], tf.int32)
    label_width = tf.cast(example['label_width'], tf.int32)
    label_depth = tf.cast(example['label_depth'], tf.int32)
    feature_raw = tf.decode_raw(example['feature_raw'], tf.uint8)
    label_raw = tf.decode_raw(example['label_raw'], tf.uint8)

    feature = tf.reshape(feature_raw, tf.stack([feature_height, feature_width, feature_depth]))
    label = tf.reshape(label_raw, tf.stack([label_height, label_width, label_depth]))

    return feature, label

# Here, we populate a file with a list of images in a directory.
def populate_file(file, file_dir):
    ''' populate file with file names ending in '.png' '''

    F = open(file, 'w')

    lst = os.listdir(file_dir)
    lst = lst.sort()

    for file in sorted(os.listdir(file_dir)):
        if file.endswith('.png'):
            F.write(file_dir + file + '\n')

    F.close()

    return

# Here, we read the file generated by populate_file().
def parse_file(file):
    ''' parse file for list of filenames'''

    F = open(file, 'r')

    file_list = F.read().splitlines()

    F.close()

    return file_list

# Here, we define a function that writes a TFRecord file.
def output_pipeline(filenames, num_epochs=1):
    """Write a TFRecord"""

    filename_queue = tf.train.string_input_producer(
        filenames,
        num_epochs=num_epochs,
        shuffle=False
    )

    reader = tf.WholeFileReader()
    _, value = reader.read(filename_queue)

    image = tf.image.decode_png(value)

    return image

# Here, we define important directories.
logs = './logs/'
model = './model/'
input_dir = './data/input/' #'./data/test/ to learn the identity kernel
output_dir = './data/output/'
training_features_dir = input_dir + 'features/training/'
training_labels_dir = input_dir + 'labels/training/'
validation_features_dir = input_dir + 'features/validation/'
validation_labels_dir = input_dir + 'labels/validation/'

# Here, we define important file names.
training_features_txt_file = input_dir + 'training_features.txt'
training_labels_txt_file = input_dir + 'training_labels.txt'
validation_features_txt_file = input_dir + 'validation_features.txt'
validation_labels_txt_file = input_dir + 'validation_labels.txt'

training_record_file = output_dir + 'training_record.tfrecords'
validation_record_file = output_dir + 'validation_record.tfrecords'

# Here, we populate the feature_file and label_file with name of feature and label images.
populate_file(training_features_txt_file, training_features_dir)
populate_file(training_labels_txt_file, training_labels_dir)
populate_file(validation_features_txt_file, validation_features_dir)
populate_file(validation_labels_txt_file, validation_labels_dir)

training_features_list = parse_file(training_features_txt_file)
training_labels_list = parse_file(training_labels_txt_file)
validation_features_list = parse_file(validation_features_txt_file)
validation_labels_list = parse_file(validation_labels_txt_file)

# Here, we create handles for reading and writing TFRecord files.
training_features = output_pipeline(training_features_list, 1)
training_labels = output_pipeline(training_labels_list, 1)
validation_features = output_pipeline(validation_features_list, 1)
validation_labels = output_pipeline(validation_labels_list, 1)

# Initialisation commands
init = [tf.global_variables_initializer(), tf.local_variables_initializer()]

# In this session, we read our raw data and create a TFRecord file.
with tf.Session() as s:

    s.run(init)

    training_writer = tf.python_io.TFRecordWriter(training_record_file)

    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)

    print('Starting...')

    try:

        i = 0

        while not coord.should_stop():

            single_training_feature = s.run(training_features)
            single_training_label = s.run(training_labels)

            training_example = make_example(single_training_feature, single_training_label)

            training_writer.write(training_example.SerializeToString())

            print(i)

            i += 1

    except tf.errors.OutOfRangeError:
        print('Done!')
    finally:
        coord.request_stop()

    coord.join(threads)

    training_writer.close()

    validation_writer = tf.python_io.TFRecordWriter(validation_record_file)

    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)

    print('Starting...')

    try:

        i = 0

        while not coord.should_stop():

            single_validation_feature = s.run(validation_features)
            single_validation_label = s.run(validation_labels)

            validation_example = make_example(single_validation_feature, single_validation_label)

            validation_writer.write(validation_example.SerializeToString())

            print(i)

            i += 1

    except tf.errors.OutOfRangeError:
        print('Done!')
    finally:
        coord.request_stop()

    coord.join(threads)

    validation_writer.close()
